DOUBTS:
virtual synchrony: order does not matter? L7 ppt51


points
virual syncrhonious system cannot scale: succeptible to partion


Paas 	: You get access to flexible computing and storage infrastructure, coupled with a software platform (often tightly coupled)
Saas
Iaas
Haas
Paxos safety?
If some round has a majority (i.e., quorum) hearing proposed value v’ and accepting it (middle of Phase 2), then subsequently at each round either	: 1) the round chooses v’ as decision or 2) the round fails •	Proof	: –	Potential leader waits for majority of OKs in Phase 1 –	At least one will contain v’ (because two majorities or quorums always intersect) –	It will choose to send out v’ in Phase 2 •	Success requires a majority, and any two majority sets intersect Scale, On-demand access, data-intensive, new programming

WUE = Annual Water Usage / IT Equipment Energy (L/kWh) – low is good • PUE = Total facility Power / IT Equipment Power – low is good
(e.g., Google~1.11) 
AlltoAll	:L=N/tg=N*logN/T
christian	:between|[t+min2, t+RTT-min1]|t + (RTT+min2-min1)/2|bounded
NTP	:o = (tr1 – tr2 + ts2 – ts1)/2|
Lamport 	: max(localClock+messagTimestamp)
vector 	: Vi[i] = Vi[i] + 1 && Vi[j] = max(Vmessage[j], Vi[j]) for j ≠ i
heckpointing	: can restart distributed application on failure
Garbage collection of objects	: objects at servers that don’t have any other objects (at any servers) with pointers to them
Deadlock detection	: Useful in database transaction systems
Termination of computation	: Useful in batch computing systems like Folding@Home, SETI@Home
FIFO	:If a correct process issues (sends) multicast(g,m) to group g and then multicast(g,m’), then every correct  process that delivers m’ would already have delivered m.


Name,Memory,Lookup,#messagesForLookup
Napster,O(1)/O(N)@server,O(1),O(1)
Chord,log(N),log(N),log(N)
Kelips,O(rootN),o(1),disseminationtime(ologn)
Safety:  For all non-faulty processes p: (p’s elected = (q: a particular non-faulty process with the best attribute value) or Null)
Liveness: For all election runs: (election run terminates) & for all non-faulty processes p: p’s elected is not Null
Election:
Ring:election|elected|worst:Best=3n-1:2n
Bull:coordinator|ok|election|worst:best=o(n)sq:
GoogleChubby:live & safe|quoram|voteOnce|masterLease|MutualExclusion_locking
__Bandwidth|#msg sent in each enter and exit.
__Clientdelay|delay incurred by a process at each enter and exit operation (when no other process is in, or waiting)
__SynchDelay|time between one process exiting the critical section and the next process entering it (when there is only one process waiting
****MutualExclusion:enter(),AccessResource(),exit()
  Safety|At most one process execute CS
  Liveness|Every request for a CS is granted eventually
  Ordering(desirable)|FIFO
CentralAlgorithm|safety+liveness|singlePointFailure|ordered@master
  Bandwidth:clientdelay:syncDelay=2 enter,1exit:2:2
RingBased|safe+live|noOrdering|
  BW:CD:SD=1,1:0-N:1-(N-1)
Ricart_Agarwala|NoToken|Multicast|LamportTime|casuality|ordered
RickartAgarwala
		||enter() at process Pi
		 set state to Wanted
		 multicast “Request” <Ti, Pi> to all processes, where Ti = current Lamport timestamp at Pi
		 wait until all processes send back “Reply”
		 change state to Held and enter the CS
		||On receipt of a Request <Tj, Pj> at Pi (i ≠ j):
		if (state = Held) or (state = Wanted & (Ti, i) < (Tj, j)) 
					add request to local queue (of waiting requests)
		else send “Reply” to Pj
		||exit() at process Pi
		 change state to Released and “Reply” to all queued requests.
  state"Wanted|Held|Released"|"Request|Reply"
  BW:CD:SD=2(N-1),(N-1):1:1
Maekawa|voting_set_k|non_emptyIntersection|votesAtMostOnce|NeedsVoteFromAll
DOES NOT GURANTEE LIVENESS|DEADLOCK|EACH WOULD VOTE FOR ITSELF
  state"Wanted|Held|Released"|"VotedFlag"|"Request|Reply|Release"
		When Pi receives a Request from Pj:
		if (state == Held OR voted = true)
			queue Request
		else
			send Reply to Pj and set voted = true

		When Pi receives a Release from Pj:
		if (queue empty)
			voted = false
		else
			dequeue head of queue, say Pk
			Send Reply only to Pk
			voted = true
  BW:CD:SD=2rootN,rootN:(N-1),1:2
FLP:SchedulesCommutative|Disjoint Schedules are commutative
s1={(p1,m1),(p2,m2),(p1,m1)}|s2={(p,m)}|p Not in s1
There always exists an initial config that is bivalent
starting from bivalent,there is always another bivalent config reachable
|Routing|NetworkLayer
|_DistanceVectorRouting(RIP)|Proactive|
|_Link-State(OSPF)
|_TCP:connectionless|decomposition|reliable:acks|mult/demult|congestion|FIFO_order
|_DNS|Recursive:may_fail_
|_Iterative_LocalNameServerMostResponsibilty>BETTER|longer but more control

RPC|CodeReuse|Dispatcher:inServer
|Concurrency|pessimistic|optimisitic
|_Pessimistic:locking(read|writeMode)|optimisticTwoPhaseLocking:serialEquivilance|GrowingPhase,ShrinkingPhase
|_Optimistic|firstCut,TimeStampOrder,MultiVersionControl
|_firstCutApproach:r/w@will-chk@commit-rollback:cascadingAborts
|_TimeStampOrder:read_if_lastW_by_lowerid,write_if_lastR_bylowerid
|_MultiVersionControl:multipleVers,readOnlyImmediatePrevious
EventualConsistency:Cassandra+Dynamo=lastWriteWins|
Riak=vectorClocks:sizeBased|TimeBasedPruning
#Replication|challenge|Transperancy+consistency
|1-f^k:K=#replica,f=probFailure:
|_Passive:Active=Master:treatAllSame
Transaction&Replication|NonReplicatedSystem:serialEquvilance
|_InReplicatedSys:serialEqui+OneCopySerializability:"""The effect of transactions performed by clients on replicated objects should be the same as if they had been performed one at a time on a single set of objects"""
TransactionCommit:OnePhase|TwoPhase|Coordinator:Prepare+Yes/No+Commit<LOG>
#KeyValueStore
|_Cassandra|ReplicationStatergy|Simple,Network
|_Simple:Random,ByteOrder<rangeQueries>|
|_NetworkTopology<DCWide+withinDC_RACK>:Snitches:Simple|RackInferring<x.<DC>.<rack>.<node>|PropertyFile|ECS:ECSRegion<DC>+AvailabilityZone<rack>
|_Writes|HintedHandOff:replicasDown:Timeout|memTable<Append>Cache|SSTABLE_disk
|_Reads|slow:BLOOM+INDEX+SSTABLE+MULTIPLEREPLICA+MULTIPLE_sstables_rows
|_PHI:toSetLongerTimeoutForSlowerServers
|_Quoram:W:Coordinator:waitsFor_W_toRepsond||asynchronous<FAST>Write&Return
|_(W+N>N__W>N/2):
|_(W=1,R=1): very few writes and reads
|_(W=N, R=1): great for read-heavy workloads
|_(W=N/2+1, R=N/2+1): great for write-heavy workloads
|_(W=1, R=N): great for write-heavy workloads with mostly one client writing per key
MongoDB|consistent,shard:collectionOfChunks
|_ReadPrefernce=primary/secondary/nearest
|_Write="acknowledged":primaryAcksImmediately|Journalled+replicaAck




